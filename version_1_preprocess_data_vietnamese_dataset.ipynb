{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "327d0d97-922e-41ae-b97f-fe4a0035963e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "RAW_DATA_DIR: str = \"data/cafef/raw_data\"\n",
    "HISTORY_LENGTH: int = 50\n",
    "TITLE_LENGTH: int = 20\n",
    "ABSTRACT_LENGTH: int = 50\n",
    "\n",
    "USER_TO_INT: Dict[str, int] = {\"PSEUDO_USER\": 0}\n",
    "POST_TO_INT: Dict[str, int] = {\"PSEUDO_POST\": 0}\n",
    "CATEGORY_TO_INT: Dict[str, int] = {\"PSEUDO_CATEGORY\": 0}\n",
    "WORD_TO_INT: Dict[str, int] = {\"PSEUDO_WORD\": 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ad42d1-3524-466a-8b87-6081a34b6b0c",
   "metadata": {},
   "source": [
    "# UTIL FUNCTIONS #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d439ed4b-733a-40dd-a65e-db25783bf6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "\n",
    "def get_int_mapping(data: object, data_to_int: Dict[object, int]) -> int:\n",
    "    if data not in data_to_int:\n",
    "        data_to_int[data] = len(data_to_int)\n",
    "    return data_to_int[data]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9535fbf8-0a61-4680-9246-71af2c548568",
   "metadata": {},
   "source": [
    "# PROCESS BEHAVIOUR DF #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2acff2ee-a225-4e14-9b8b-134ae0b1c673",
   "metadata": {},
   "source": [
    "## Process functions ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d71ef15-caa3-44b0-a9d6-5b94f5e17a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from typing import List, Dict, Tuple\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "\n",
    "def process_behaviour_df(df: pd.DataFrame) -> List[Dict]:\n",
    "    def _extract_history(_x: str) -> List[str]:\n",
    "        if len(_x) == 0:\n",
    "            return []\n",
    "        _post_ids: List[str] = _x.split(\" \")\n",
    "        _post_ids: List[str] = _post_ids[:HISTORY_LENGTH]\n",
    "        return _post_ids\n",
    "    \n",
    "    def _extract_positive_negative(_x: str) -> Tuple[List[str], List[str]]:\n",
    "        _positive: List[str] = []\n",
    "        _negative: List[str] = []\n",
    "        for _post_label in _x.split(\" \"):\n",
    "            _post_id, _label = _post_label.split(\"-\")\n",
    "            assert _label in {\"1\", \"0\"}\n",
    "            if _label == \"1\":\n",
    "                _positive.append(_post_id)\n",
    "            else:\n",
    "                _negative.append(_post_id)\n",
    "        return _positive, _negative\n",
    "    \n",
    "    df.fillna(value=\"\", inplace=True)\n",
    "    df[\"timestamp\"] = df.timestamp.apply(lambda x: datetime.strptime(x, \"%m/%d/%Y %I:%M:%S %p\"))\n",
    "    df[\"history\"] = df.history.apply(_extract_history)\n",
    "    df[\"impression\"] = df.impression.apply(_extract_positive_negative)\n",
    "    \n",
    "    result: List[Dict] = []\n",
    "    progress_bar = tqdm(df.itertuples(index=False), desc=\"Processing behaviours data... \")\n",
    "    for row in progress_bar:\n",
    "        user_id: str = row.user_id\n",
    "        timestamp: datetime = row.timestamp\n",
    "        history: List[str] = row.history\n",
    "        positive, negative = row.impression\n",
    "\n",
    "        user_id: int = get_int_mapping(data=user_id, data_to_int=USER_TO_INT)\n",
    "        history: List[int] = list(map(lambda x: get_int_mapping(data=x, data_to_int=POST_TO_INT), history)) \n",
    "        positive: List[int] = list(map(lambda x: get_int_mapping(data=x, data_to_int=POST_TO_INT), positive)) \n",
    "        negative: List[int] = list(map(lambda x: get_int_mapping(data=x, data_to_int=POST_TO_INT), negative)) \n",
    "\n",
    "        result.append({\"user_id\": user_id, \"timestamp\": timestamp,\n",
    "                       \"history\": history, \"positive\": positive, \"negative\": negative})\n",
    "    progress_bar.close()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac94befa-fc02-4964-bf17-6aadb13d6b42",
   "metadata": {},
   "source": [
    "## Process data ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eaa6ca88-7419-4862-96e3-06b4c32e12af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing behaviours data... : 68999it [00:02, 23510.67it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from typing import List, Dict\n",
    "\n",
    "\n",
    "TRAIN_BEHAVIOUR_DF: pd.DataFrame = pd.read_csv(f\"{RAW_DATA_DIR}/train/behaviours.tsv\", sep='\\t', header=None, index_col=0)\n",
    "TRAIN_BEHAVIOUR_DF.columns = [\"user_id\", \"timestamp\", \"history\", \"impression\"]\n",
    "TRAIN_BEHAVIOURS: List[Dict] = process_behaviour_df(df=TRAIN_BEHAVIOUR_DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1121f59d-6f5c-47c6-9dfa-b506a9e28248",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing behaviours data... : 27397it [00:01, 20700.57it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from typing import List, Dict\n",
    "\n",
    "\n",
    "DEV_BEHAVIOUR_DF: pd.DataFrame = pd.read_csv(f\"{RAW_DATA_DIR}/dev/behaviours.tsv\", sep='\\t', header=None, index_col=0)\n",
    "DEV_BEHAVIOUR_DF.columns = [\"user_id\", \"timestamp\", \"history\", \"impression\"]\n",
    "DEV_BEHAVIOURS: List[Dict] = process_behaviour_df(df=DEV_BEHAVIOUR_DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f866741-5640-4179-a6ad-c8aa0a0d712b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing behaviours data... : 26948it [00:01, 20275.24it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from typing import List, Dict\n",
    "\n",
    "\n",
    "TEST_BEHAVIOUR_DF: pd.DataFrame = pd.read_csv(f\"{RAW_DATA_DIR}/test/behaviours.tsv\", sep='\\t', header=None, index_col=0)\n",
    "TEST_BEHAVIOUR_DF.columns = [\"user_id\", \"timestamp\", \"history\", \"impression\"]\n",
    "TEST_BEHAVIOURS: List[Dict] = process_behaviour_df(df=TEST_BEHAVIOUR_DF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589593c2-cf54-4fcf-9c72-f05388363fe9",
   "metadata": {},
   "source": [
    "## Save data ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11f61349-0003-4224-8c19-86f433c59a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN BEHAVIOURS SIZE: 68999;   DEV SIZE: 27397;    TEST SIZE: 26948\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from common.utils import PickleWriteObjectToLocalPatient\n",
    "\n",
    "\n",
    "print(f\"TRAIN BEHAVIOURS SIZE: {len(TRAIN_BEHAVIOURS)};   DEV SIZE: {len(DEV_BEHAVIOURS)};    TEST SIZE: {len(TEST_BEHAVIOURS)}\")\n",
    "\n",
    "PickleWriteObjectToLocalPatient().write(x=TRAIN_BEHAVIOURS, file_name=\"data/cafef/train/behaviours.pkl\")\n",
    "PickleWriteObjectToLocalPatient().write(x=DEV_BEHAVIOURS, file_name=\"data/cafef/dev/behaviours.pkl\")\n",
    "PickleWriteObjectToLocalPatient().write(x=TEST_BEHAVIOURS, file_name=\"data/cafef/test/behaviours.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "375bfa2c-cc3f-46c6-b626-b547b4c691fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL USERS: 66434, TOTAL POSTS: 58797\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from common.utils import PickleWriteObjectToLocalPatient\n",
    "\n",
    "\n",
    "print(f\"TOTAL USERS: {len(USER_TO_INT)}, TOTAL POSTS: {len(POST_TO_INT)}\")\n",
    "PickleWriteObjectToLocalPatient().write(x=USER_TO_INT, file_name=\"data/cafef/object_to_int/user_to_int.pkl\")\n",
    "PickleWriteObjectToLocalPatient().write(x=POST_TO_INT, file_name=\"data/cafef/object_to_int/post_to_int.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cf5f4d-96f8-42cf-93b9-e148d89259f8",
   "metadata": {},
   "source": [
    "# PROCESS POST DF #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd09ff2-3c58-460b-82e7-7089f1867662",
   "metadata": {},
   "source": [
    "## Process functions ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82e51e8d-2acb-45f2-baf3-006ab7c8de24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Some weights of the model checkpoint at vinai/phobert-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING DEVICE cuda:1\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from common.utils import LocalFileHandlerUtils\n",
    "\n",
    "\n",
    "LocalFileHandlerUtils.check_and_make_directory(directory=\"pretrained_data/vinai/phobert-base/tokenizer\")\n",
    "LocalFileHandlerUtils.check_and_make_directory(directory=\"pretrained_data/vinai/phobert-base/model\")\n",
    "BERT_TOKENIZER = AutoTokenizer.from_pretrained(\"vinai/phobert-base\", cache_dir=\"pretrained_data/vinai/phobert-base/tokenizer\")\n",
    "BERT_MODEL = AutoModel.from_pretrained(\"vinai/phobert-base\", cache_dir=\"pretrained_data/vinai/phobert-base/model\")\n",
    "DEVICE = torch.device(\"cuda:1\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "BERT_MODEL = BERT_MODEL.to(DEVICE)\n",
    "print(f\"USING DEVICE {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45971578-ecc1-47bb-ae35-463df80a4cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "from torch import Tensor\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_batch_bert_encodes(list_text: List[str], sequence_len: int) -> np.ndarray:\n",
    "    tokenize_result: Dict = BERT_TOKENIZER(text=list_text, padding=\"max_length\", max_length=sequence_len, truncation=True, return_tensors=\"pt\")\n",
    "    token_ids: Tensor = tokenize_result[\"input_ids\"]\n",
    "    attention_mask: Tensor = tokenize_result[\"attention_mask\"]\n",
    "    assert token_ids.shape == (len(list_text), sequence_len)\n",
    "    assert attention_mask.shape == (len(list_text), sequence_len)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        BERT_MODEL.eval()\n",
    "        output: Tensor = BERT_MODEL(input_ids=token_ids.to(DEVICE), attention_mask=attention_mask.to(DEVICE)).pooler_output\n",
    "        output: np.ndarray = output.cpu().detach().numpy()\n",
    "\n",
    "    assert output.shape == (len(list_text), 768)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b3800a7-1c6a-4912-bade-7080d773d9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vncorenlp import VnCoreNLP\n",
    "from typing import List\n",
    "\n",
    "\n",
    "VNCORE_PREPROCESSOR = VnCoreNLP(address=\"http://10.5.1.230\", port=2811)\n",
    "\n",
    "\n",
    "def get_clean_text(text: str) -> str:\n",
    "    sentence_words: List[List[str]] = VNCORE_PREPROCESSOR.tokenize(text=text)\n",
    "    words: List[str] = [word for sentence_word in sentence_words for word in sentence_word]\n",
    "    text: str = \" \".join(words)\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "453c159b-d8c5-4bf2-963d-4d9a22eb62b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_ids(text: str, sequence_len: int, is_training: bool) -> List[str]:\n",
    "    result: List[int] = list(map(lambda x: 0, range(sequence_len)))\n",
    "    words: List[str] = text.split(\" \")\n",
    "    for index, word in enumerate(words):\n",
    "        if index == sequence_len:\n",
    "            break\n",
    "        if is_training:\n",
    "            result[index] = get_int_mapping(data=word, data_to_int=WORD_TO_INT)\n",
    "        elif word in WORD_TO_INT:\n",
    "            result[index] = WORD_TO_INT[word]\n",
    "    \n",
    "    assert len(result) == sequence_len\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2098a00a-8691-4512-b7af-9c17ba36a61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "\n",
    "def process_post_df(df: pd.DataFrame, is_training: bool) -> Dict[int, Dict]:\n",
    "    result: Dict[int, Dict] = {}\n",
    "    df.fillna(value=\"\", inplace=True)\n",
    "    \n",
    "    progress_bar = tqdm(df.itertuples(index=False), \"Processing posts... \")\n",
    "    for row in progress_bar:\n",
    "        post_id: str = row.post_id\n",
    "        if post_id not in POST_TO_INT:\n",
    "            continue\n",
    "        post_id: int = POST_TO_INT[post_id]\n",
    "        category: str = row.category\n",
    "        subcategory: str = row.subcategory\n",
    "        subcategory_name: str = row.subcategory_name\n",
    "        title: str = get_clean_text(text=row.title)\n",
    "        abstract: str = get_clean_text(text=row.abstract)\n",
    "        \n",
    "        title_token_ids: List[int] = get_token_ids(text=title, sequence_len=TITLE_LENGTH, is_training=is_training)\n",
    "        abstract_token_ids: List[int] = get_token_ids(text=abstract, sequence_len=ABSTRACT_LENGTH, is_training=is_training)\n",
    "\n",
    "        if is_training:\n",
    "            category_id: int = get_int_mapping(data=category, data_to_int=CATEGORY_TO_INT)\n",
    "            subcategory_id: int = get_int_mapping(data=subcategory, data_to_int=CATEGORY_TO_INT)\n",
    "        else:\n",
    "            category_id: int = CATEGORY_TO_INT.get(category, 0)\n",
    "            subcategory_id: int = CATEGORY_TO_INT.get(subcategory, 0)\n",
    "\n",
    "        result[post_id] = {\"title\": title, \"abstract\": abstract,\n",
    "                           \"title_token_ids\": title_token_ids, \"abstract_token_ids\": abstract_token_ids,\n",
    "                           \"category\": category, \"subcategory\": subcategory, \"subcategory_name\": subcategory_name,\n",
    "                           \"category_id\": category_id, \"subcategory_id\": subcategory_id}\n",
    "    progress_bar.close()\n",
    "    \n",
    "    \n",
    "    list_info: List[Dict] = list(result.values())\n",
    "    batch_size: int = 128\n",
    "    progress_bar = tqdm(range(0, len(list_info), batch_size), \"Updating bert encode\")\n",
    "    for start_index in progress_bar:\n",
    "        end_index: int = min(start_index+batch_size, len(list_info))\n",
    "        batch_info: List[Dict] = list_info[start_index:end_index]\n",
    "        \n",
    "        titles: List[str] = list(map(lambda x: x[\"title\"], batch_info))\n",
    "        abstracts: List[str] = list(map(lambda x: x[\"abstract\"], batch_info))\n",
    "        \n",
    "        batch_title_bert_encode: np.ndarray = get_batch_bert_encodes(list_text=titles, sequence_len=TITLE_LENGTH)\n",
    "        batch_abstract_bert_encode: np.ndarray = get_batch_bert_encodes(list_text=abstracts, sequence_len=ABSTRACT_LENGTH)\n",
    "        \n",
    "        for index, info in enumerate(batch_info):\n",
    "            info[\"title_bert_encode\"] = batch_title_bert_encode[index]\n",
    "            info[\"abstract_bert_encode\"] = batch_abstract_bert_encode[index]\n",
    "    progress_bar.close()\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aece8ef3-e9e1-4d66-bb12-d88bd9236574",
   "metadata": {},
   "source": [
    "## Process data ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad773e5-fcf0-47b0-b2a8-d4f6a3cc8e34",
   "metadata": {},
   "source": [
    "### Train ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3eb8049a-dc5d-416f-b968-f5176a0f3b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2415524/408964673.py:6: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  TRAIN_POST_DF = TRAIN_POST_DF.append(pseudo_post_row, ignore_index=True)\n",
      "Processing posts... : 46732it [05:44, 135.65it/s]\n",
      "Updating bert encode: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 365/365 [01:03<00:00,  5.74it/s]\n"
     ]
    }
   ],
   "source": [
    "############################ Read data ###############################\n",
    "pseudo_post_row: Dict = {\"post_id\": \"PSEUDO_POST\", \"category\": \"PSEUDO_CATEGORY\", \"subcategory\": \"PSEUDO_CATEGORY\", \"subcategory_name\": \"PSEUDO_CATEGORY\", \"title\": \"\", \"abstract\": \"\"}\n",
    "\n",
    "TRAIN_POST_DF = pd.read_csv(f\"{RAW_DATA_DIR}/train/news.tsv\", sep='\\t', header=None, index_col=0)\n",
    "TRAIN_POST_DF.columns = [\"post_id\", \"category\", \"subcategory\", \"subcategory_name\", \"title\", \"abstract\"]\n",
    "TRAIN_POST_DF = TRAIN_POST_DF.append(pseudo_post_row, ignore_index=True)\n",
    "TRAIN_POST_DF = TRAIN_POST_DF.astype(\"str\")\n",
    "TRAIN_POST_DF.fillna(value=\"\", inplace=True)\n",
    "\n",
    "############################## Process data ################################\n",
    "TRAIN_POST_ID_TO_INFO: Dict[int, Dict] = process_post_df(df=TRAIN_POST_DF, is_training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af6c1ad2-2d60-46cc-8e33-54b3be66b7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "\n",
    "VECTORIZER = CountVectorizer(min_df=2, binary=True)\n",
    "train_documents: List[str] = list(map(lambda x: x[\"title\"] + \" \" + x[\"abstract\"], TRAIN_POST_ID_TO_INFO.values()))\n",
    "TRAIN_X: csr_matrix = VECTORIZER.fit_transform(train_documents)\n",
    "for index, info in enumerate(TRAIN_POST_ID_TO_INFO.values()):\n",
    "    info['content_bow'] = TRAIN_X[index]\n",
    "WORDS_CO_OCCUR: csr_matrix = TRAIN_X.T * TRAIN_X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2104962c-4568-475d-8496-9d187ec6e504",
   "metadata": {},
   "source": [
    "### Dev ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "092998bf-32c8-4d33-8b19-ba5b2ee07dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2415524/3302243461.py:8: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  DEV_POST_DF = DEV_POST_DF.append(pseudo_post_row, ignore_index=True)\n",
      "Processing posts... : 26405it [03:03, 143.83it/s]\n",
      "Updating bert encode: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 207/207 [00:45<00:00,  4.59it/s]\n"
     ]
    }
   ],
   "source": [
    "from common.utils import JsonReadObjectFromLocalPatient\n",
    "\n",
    "\n",
    "pseudo_post_row: Dict = {\"post_id\": \"PSEUDO_POST\", \"category\": \"PSEUDO_CATEGORY\", \"subcategory\": \"PSEUDO_CATEGORY\", \"subcategory_name\": \"PSEUDO_CATEGORY\", \"title\": \"\", \"abstract\": \"\"}\n",
    "\n",
    "DEV_POST_DF = pd.read_csv(f\"{RAW_DATA_DIR}/dev/news.tsv\", sep='\\t', header=None, index_col=0)\n",
    "DEV_POST_DF.columns = [\"post_id\", \"category\", \"subcategory\", \"subcategory_name\", \"title\", \"abstract\"]\n",
    "DEV_POST_DF = DEV_POST_DF.append(pseudo_post_row, ignore_index=True)\n",
    "DEV_POST_DF = DEV_POST_DF.astype(\"str\")\n",
    "DEV_POST_DF.fillna(value=\"\", inplace=True)\n",
    "\n",
    "############################## Process data ################################\n",
    "DEV_POST_ID_TO_INFO: Dict[int, Dict] = process_post_df(df=DEV_POST_DF, is_training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "88375022-cebb-4d54-b3f8-644cbcea7bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_documents: List[str] = list(map(lambda x: x[\"title\"] + \" \" + x[\"abstract\"], DEV_POST_ID_TO_INFO.values()))\n",
    "DEV_X: csr_matrix = VECTORIZER.transform(dev_documents)\n",
    "for index, info in enumerate(DEV_POST_ID_TO_INFO.values()):\n",
    "    info['content_bow'] = DEV_X[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c39ea1-5538-4d4a-9483-892f7888884c",
   "metadata": {},
   "source": [
    "### Test ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a5b4ceef-06f6-4156-9012-e674e06b5dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2415524/2278326439.py:8: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  TEST_POST_DF = TEST_POST_DF.append(pseudo_post_row, ignore_index=True)\n",
      "Processing posts... : 27507it [03:21, 136.84it/s]\n",
      "Updating bert encode: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 215/215 [00:34<00:00,  6.30it/s]\n"
     ]
    }
   ],
   "source": [
    "from common.utils import JsonReadObjectFromLocalPatient\n",
    "\n",
    "\n",
    "pseudo_post_row: Dict = {\"post_id\": \"PSEUDO_POST\", \"category\": \"PSEUDO_CATEGORY\", \"subcategory\": \"PSEUDO_CATEGORY\", \"subcategory_name\": \"PSEUDO_CATEGORY\", \"title\": \"\", \"abstract\": \"\"}\n",
    "\n",
    "TEST_POST_DF = pd.read_csv(f\"{RAW_DATA_DIR}/test/news.tsv\", sep='\\t', header=None, index_col=0)\n",
    "TEST_POST_DF.columns = [\"post_id\", \"category\", \"subcategory\", \"subcategory_name\", \"title\", \"abstract\"]\n",
    "TEST_POST_DF = TEST_POST_DF.append(pseudo_post_row, ignore_index=True)\n",
    "TEST_POST_DF = TEST_POST_DF.astype(\"str\")\n",
    "TEST_POST_DF.fillna(value=\"\", inplace=True)\n",
    "\n",
    "############################## Process data ################################\n",
    "TEST_POST_ID_TO_INFO: Dict[int, Dict] = process_post_df(df=TEST_POST_DF, is_training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4d42a2fb-0218-4709-a83e-ae56670e4ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_documents: List[str] = list(map(lambda x: x[\"title\"] + \" \" + x[\"abstract\"], TEST_POST_ID_TO_INFO.values()))\n",
    "TEST_X: csr_matrix = VECTORIZER.transform(test_documents)\n",
    "for index, info in enumerate(TEST_POST_ID_TO_INFO.values()):\n",
    "    info['content_bow'] = TEST_X[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0240ea4f-b158-4813-afc2-bece9f88b31c",
   "metadata": {},
   "source": [
    "## Save data ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "335592e2-4770-40c7-984c-8316a90f4dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUMBER OF CATEGORIES: 20; NUMBER OF WORDS IN TITLE + ABSTRACT: 48346\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from common.utils import PickleWriteObjectToLocalPatient\n",
    "\n",
    "\n",
    "print(f\"NUMBER OF CATEGORIES: {len(CATEGORY_TO_INT)}; NUMBER OF WORDS IN TITLE + ABSTRACT: {len(WORD_TO_INT)}\")\n",
    "PickleWriteObjectToLocalPatient().write(x=CATEGORY_TO_INT, file_name=\"data/cafef/object_to_int/category_to_int.pkl\")\n",
    "PickleWriteObjectToLocalPatient().write(x=WORD_TO_INT, file_name=\"data/cafef/object_to_int/word_to_int.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5aea714e-8aea-4d41-b928-2be74804bd9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUMBER OF WORDS IN CONTENT: 22091\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from common.utils import PickleWriteObjectToLocalPatient\n",
    "\n",
    "\n",
    "print(f\"NUMBER OF WORDS IN CONTENT: {len(VECTORIZER.vocabulary_)}\")\n",
    "PickleWriteObjectToLocalPatient().write(x=VECTORIZER, file_name=\"data/cafef/vectorizer/vectorizer.pkl\")\n",
    "PickleWriteObjectToLocalPatient().write(x=WORDS_CO_OCCUR, file_name=\"data/cafef/vectorizer/words_co_occur.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d5c513e4-9ef8-4559-bc43-aa623d3a4ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUMBER OF POSTS IN TRAIN SET: 46644; DEV SET: 26369; TEST SET: 27452\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from common.utils import PickleWriteObjectToLocalPatient\n",
    "\n",
    "\n",
    "print(f\"NUMBER OF POSTS IN TRAIN SET: {len(TRAIN_POST_ID_TO_INFO)}; DEV SET: {len(DEV_POST_ID_TO_INFO)}; TEST SET: {len(TEST_POST_ID_TO_INFO)}\")\n",
    "PickleWriteObjectToLocalPatient().write(x=TRAIN_POST_ID_TO_INFO, file_name=\"data/cafef/train/post_id_to_info.pkl\")\n",
    "PickleWriteObjectToLocalPatient().write(x=DEV_POST_ID_TO_INFO, file_name=\"data/cafef/dev/post_id_to_info.pkl\")\n",
    "PickleWriteObjectToLocalPatient().write(x=TEST_POST_ID_TO_INFO, file_name=\"data/cafef/test/post_id_to_info.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fc8367-1d0d-47e3-a6d2-0b1020c60ffc",
   "metadata": {},
   "source": [
    "# READING PRETRAINED WORD2VEC #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e219541-8ae9-41c8-9041-c4733540a57b",
   "metadata": {},
   "source": [
    "## Reading functions ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a99edcf1-5728-40b2-97f2-4a94838e4cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRETRAINED_GLOVE_FILE: str = \"/data/pretrained/phow2v/word2vec_vi_words_300dims.txt\"\n",
    "\n",
    "\n",
    "def load_word_embedding(word_to_int: Dict[str, int]) -> np.ndarray:\n",
    "    vocab_size: int = len(word_to_int)\n",
    "    num_match_words: int = 0\n",
    "    word_embedding = np.zeros(shape=(vocab_size, 300))\n",
    "    with open(PRETRAINED_GLOVE_FILE, mode=\"r\", buffering=100000, encoding=\"utf-8\") as file_obj:\n",
    "        file_obj.readline()\n",
    "        progress_bar = tqdm(file_obj.readlines(), desc=\"Reading word embedding data...\")\n",
    "        for line in progress_bar:\n",
    "            try:\n",
    "                parts: List[str] = line.strip().split(\" \")\n",
    "                word: str = parts[0]\n",
    "                if word in word_to_int:\n",
    "                    num_match_words += 1\n",
    "                    index: int = word_to_int[word]\n",
    "                    word_embedding[index] = np.array([float(v) for v in parts[1:301]])\n",
    "            except Exception as ex:\n",
    "                print(f\"Something wrong occurs: {ex}\")\n",
    "    print(f\"THERE ARE {num_match_words} WORDS OVER {vocab_size} WORDS HAVE PRETRAINED EMBEDDING\")\n",
    "    return word_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6cc11da-79aa-4aa1-a7dc-a60fb13bd708",
   "metadata": {},
   "source": [
    "## Read word embedding ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "44669925-2aeb-4ad5-a49e-bcdc3af905fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading word embedding data...: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1587507/1587507 [00:53<00:00, 29453.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THERE ARE 27923 WORDS OVER 48346 WORDS HAVE PRETRAINED EMBEDDING\n"
     ]
    }
   ],
   "source": [
    "TITLE_ABSTRACT_WORD_EMBEDDING = load_word_embedding(word_to_int=WORD_TO_INT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f81ac6f2-2bbe-4d70-af43-b1ab960d3792",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading word embedding data...: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1587507/1587507 [00:49<00:00, 32334.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THERE ARE 17093 WORDS OVER 22091 WORDS HAVE PRETRAINED EMBEDDING\n"
     ]
    }
   ],
   "source": [
    "CONTENT_WORD_EMBEDDING = load_word_embedding(word_to_int=VECTORIZER.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759ec275-8d3f-40b7-9c90-d44a5e733c1f",
   "metadata": {},
   "source": [
    "## Save data ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e36fe3b4-b3c3-499f-8d48-8e7c723f819b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from common.utils import PickleWriteObjectToLocalPatient\n",
    "\n",
    "\n",
    "PickleWriteObjectToLocalPatient().write(x=TITLE_ABSTRACT_WORD_EMBEDDING, file_name=\"data/cafef/pretrained/title_abstract_word_embedding.pkl\")\n",
    "PickleWriteObjectToLocalPatient().write(x=CONTENT_WORD_EMBEDDING, file_name=\"data/cafef/pretrained/content_word_embedding.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "6718cf4ad78129df1b1eee605c27a486cdacf4913d4ca86ea45fb6ece534069b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
