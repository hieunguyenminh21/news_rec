{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "327d0d97-922e-41ae-b97f-fe4a0035963e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "PRETRAINED_WORD_EMBEDDING_FILE: str = \"/data1/nganv/ResearchThayLinh/data/pretrained/phow2v/word2vec_vi_words_300dims.txt\"\n",
    "\n",
    "VNCORE_ADDRESS: str = \"http://10.5.1.230\"\n",
    "VNCORE_PORT: int = 2811\n",
    "\n",
    "RAW_DATA_DIR: str = \"data/version_2/soha/raw_data\"\n",
    "CLEAN_DATA_DIR: str = \"data/version_3/soha/clean_data\"\n",
    "HISTORY_LENGTH: int = 50\n",
    "TITLE_LENGTH: int = 20\n",
    "ABSTRACT_LENGTH: int = 50\n",
    "MIN_HISTORY_LENGTH: int = 5\n",
    "\n",
    "USER_TO_INT: Dict[str, int] = {\"PSEUDO_USER\": 0}\n",
    "POST_TO_INT: Dict[str, int] = {\"PSEUDO_POST\": 0}\n",
    "CATEGORY_TO_INT: Dict[str, int] = {\"PSEUDO_CATEGORY\": 0}\n",
    "WORD_TO_INT: Dict[str, int] = {\"PSEUDO_WORD\": 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ad42d1-3524-466a-8b87-6081a34b6b0c",
   "metadata": {},
   "source": [
    "# UTIL FUNCTIONS #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d439ed4b-733a-40dd-a65e-db25783bf6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "\n",
    "def get_int_mapping(data: object, data_to_int: Dict[object, int]) -> int:\n",
    "    if data not in data_to_int:\n",
    "        data_to_int[data] = len(data_to_int)\n",
    "    return data_to_int[data]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9535fbf8-0a61-4680-9246-71af2c548568",
   "metadata": {},
   "source": [
    "# PROCESS DATASET DF #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2acff2ee-a225-4e14-9b8b-134ae0b1c673",
   "metadata": {},
   "source": [
    "## Process behaviour df functions ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d71ef15-caa3-44b0-a9d6-5b94f5e17a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from typing import List, Dict, Tuple\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "\n",
    "def extract_history(x: str) -> List[str]:\n",
    "    if len(x) == 0:\n",
    "        return []\n",
    "    post_ids: List[str] = x.split(\" \")\n",
    "    post_ids: List[str] = post_ids[:HISTORY_LENGTH]\n",
    "    return post_ids\n",
    "    \n",
    "    \n",
    "def extract_positive_negative(x: str) -> Tuple[List[str], List[str]]:\n",
    "    positive: List[str] = []\n",
    "    negative: List[str] = []\n",
    "    for post_label in x.split(\" \"):\n",
    "        post_id, label = post_label.split(\"-\")\n",
    "        assert label in {\"1\", \"0\"}\n",
    "        if label == \"1\":\n",
    "            positive.append(post_id)\n",
    "        else:\n",
    "            negative.append(post_id)\n",
    "    return positive, negative\n",
    "    \n",
    "\n",
    "def process_behaviour_df(df: pd.DataFrame) -> List[Dict]:\n",
    "    df.columns = [\"user_id\", \"timestamp\", \"history\", \"impression\"]\n",
    "    df = df.astype(\"str\")\n",
    "    df.fillna(value=\"\", inplace=True)\n",
    "    \n",
    "    df[\"timestamp\"] = df.timestamp.apply(lambda x: datetime.strptime(x, \"%m/%d/%Y %I:%M:%S %p\"))\n",
    "    df[\"history\"] = df.history.apply(extract_history)\n",
    "    df[\"impression\"] = df.impression.apply(extract_positive_negative)\n",
    "    \n",
    "    result: List[Dict] = []\n",
    "    progress_bar = tqdm(df.itertuples(index=False), desc=\"Processing behaviours data... \")\n",
    "    for row in progress_bar:\n",
    "        user_id: str = row.user_id\n",
    "        timestamp: datetime = row.timestamp\n",
    "        history: List[str] = row.history\n",
    "        if len(history) < MIN_HISTORY_LENGTH:\n",
    "            continue\n",
    "        positive, negative = row.impression\n",
    "\n",
    "        user_id: int = get_int_mapping(data=user_id, data_to_int=USER_TO_INT)\n",
    "        history: List[int] = list(map(lambda x: get_int_mapping(data=x, data_to_int=POST_TO_INT), history)) \n",
    "        positive: List[int] = list(map(lambda x: get_int_mapping(data=x, data_to_int=POST_TO_INT), positive)) \n",
    "        negative: List[int] = list(map(lambda x: get_int_mapping(data=x, data_to_int=POST_TO_INT), negative)) \n",
    "\n",
    "        result.append({\"user_id\": user_id, \"timestamp\": timestamp,\n",
    "                       \"history\": history, \"positive\": positive, \"negative\": negative})\n",
    "    progress_bar.close()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fd8d78-5491-45f8-929a-542ad698006e",
   "metadata": {},
   "source": [
    "## Process post df functions ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b3800a7-1c6a-4912-bade-7080d773d9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vncorenlp import VnCoreNLP\n",
    "from typing import List, Dict\n",
    "\n",
    "\n",
    "VNCORE_PREPROCESSOR = VnCoreNLP(address=VNCORE_ADDRESS, port=VNCORE_PORT)\n",
    "\n",
    "\n",
    "def get_clean_text(text: str) -> str:\n",
    "    sentence_words: List[List[str]] = VNCORE_PREPROCESSOR.tokenize(text=text)\n",
    "    words: List[str] = [word for sentence_word in sentence_words for word in sentence_word]\n",
    "    text: str = \" \".join(words)\n",
    "    return text.lower()\n",
    "\n",
    "\n",
    "def get_token_ids(text: str, sequence_len: int, is_training: bool) -> List[int]:\n",
    "    result: List[int] = [0 for _ in range(sequence_len)]\n",
    "    words: List[str] = text.split(\" \")\n",
    "    for index, word in enumerate(words):\n",
    "        if index == sequence_len:\n",
    "            break\n",
    "        if is_training:\n",
    "            result[index] = get_int_mapping(data=word, data_to_int=WORD_TO_INT)\n",
    "        elif word in WORD_TO_INT:\n",
    "            result[index] = WORD_TO_INT[word]\n",
    "    assert len(result) == sequence_len\n",
    "    return result\n",
    "\n",
    "\n",
    "def process_post_df(df: pd.DataFrame, is_training: bool) -> Dict[int, Dict]:\n",
    "    df.columns = [\"post_id\", \"category\", \"subcategory\", \"subcategory_name\", \"title\", \"abstract\"]\n",
    "    pseudo_post_row: Dict = {\n",
    "        \"post_id\": \"PSEUDO_POST\", \n",
    "        \"category\": \"PSEUDO_CATEGORY\", \"subcategory\": \"PSEUDO_CATEGORY\", \n",
    "        \"subcategory_name\": \"PSEUDO_CATEGORY\", \n",
    "        \"title\": \"\", \"abstract\": \"\"\n",
    "    }\n",
    "    df = df.append(pseudo_post_row, ignore_index=True)\n",
    "    df = df.astype(\"str\")\n",
    "    df.fillna(value=\"\", inplace=True)\n",
    "\n",
    "    result: Dict[int, Dict] = {}\n",
    "    progress_bar = tqdm(df.itertuples(index=False), \"Processing posts... \")\n",
    "    for row in progress_bar:\n",
    "        post_id: str = row.post_id\n",
    "        if post_id not in POST_TO_INT:\n",
    "            continue\n",
    "        post_id: int = POST_TO_INT[post_id]\n",
    "        category: str = row.category\n",
    "        subcategory: str = row.subcategory\n",
    "        subcategory_name: str = row.subcategory_name\n",
    "        title: str = get_clean_text(text=row.title)\n",
    "        abstract: str = get_clean_text(text=row.abstract)\n",
    "        \n",
    "        title_token_ids: List[int] = get_token_ids(text=title, sequence_len=TITLE_LENGTH, is_training=is_training)\n",
    "        abstract_token_ids: List[int] = get_token_ids(text=abstract, sequence_len=ABSTRACT_LENGTH, is_training=is_training)\n",
    "\n",
    "        if is_training:\n",
    "            category_id: int = get_int_mapping(data=category, data_to_int=CATEGORY_TO_INT)\n",
    "            subcategory_id: int = get_int_mapping(data=subcategory, data_to_int=CATEGORY_TO_INT)\n",
    "        else:\n",
    "            category_id: int = CATEGORY_TO_INT.get(category, 0)\n",
    "            subcategory_id: int = CATEGORY_TO_INT.get(subcategory, 0)\n",
    "\n",
    "        result[post_id] = {\"title\": title, \"abstract\": abstract,\n",
    "                           \"title_token_ids\": title_token_ids, \"abstract_token_ids\": abstract_token_ids,\n",
    "                           \"category\": category, \"subcategory\": subcategory, \"subcategory_name\": subcategory_name,\n",
    "                           \"category_id\": category_id, \"subcategory_id\": subcategory_id}\n",
    "    progress_bar.close()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee55635c-1f63-43e1-9d51-3e50f4b696fa",
   "metadata": {},
   "source": [
    "# READING WORD EMBEDDING #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61f8e5dd-3473-4a56-aa2f-3ecf1bf015d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def load_word_embedding() -> np.ndarray:\n",
    "    vocab_size: int = len(WORD_TO_INT)\n",
    "    num_match_words: int = 0\n",
    "    word_embedding = np.zeros(shape=(vocab_size, 300))\n",
    "    with open(PRETRAINED_WORD_EMBEDDING_FILE, mode=\"r\", \n",
    "              buffering=100000, encoding=\"utf-8\") as file_obj:\n",
    "        file_obj.readline()\n",
    "        progress_bar = tqdm(file_obj, desc=\"Reading word embedding data...\")\n",
    "        for line in progress_bar:\n",
    "            try:\n",
    "                parts: List[str] = line.strip().split(\" \")\n",
    "                word: str = parts[0]\n",
    "                if word in WORD_TO_INT:\n",
    "                    num_match_words += 1\n",
    "                    index: int = WORD_TO_INT[word]\n",
    "                    word_embedding[index] = np.array([float(v) for v in parts[1:301]])\n",
    "            except Exception as ex:\n",
    "                print(f\"Something wrong occurs: {ex}\")\n",
    "    print(f\"THERE ARE {num_match_words} WORDS OVER {vocab_size} WORDS HAVE PRETRAINED EMBEDDING\")\n",
    "    return word_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac94befa-fc02-4964-bf17-6aadb13d6b42",
   "metadata": {},
   "source": [
    "## EXECUTE PROCESS DATA ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eaa6ca88-7419-4862-96e3-06b4c32e12af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing behaviours data... : 407740it [00:46, 8834.72it/s] \n",
      "Processing behaviours data... : 125111it [00:17, 7114.60it/s] \n",
      "Processing behaviours data... : 119599it [00:12, 9320.00it/s] \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from typing import List, Dict\n",
    "\n",
    "\n",
    "TRAIN_BEHAVIOUR_DF: pd.DataFrame = pd.read_csv(f\"{RAW_DATA_DIR}/train/behaviours.tsv\", sep='\\t', header=None, index_col=0)\n",
    "TRAIN_BEHAVIOURS: List[Dict] = process_behaviour_df(df=TRAIN_BEHAVIOUR_DF)\n",
    "\n",
    "DEV_BEHAVIOUR_DF: pd.DataFrame = pd.read_csv(f\"{RAW_DATA_DIR}/dev/behaviours.tsv\", sep='\\t', header=None, index_col=0)\n",
    "DEV_BEHAVIOURS: List[Dict] = process_behaviour_df(df=DEV_BEHAVIOUR_DF)\n",
    "\n",
    "TEST_BEHAVIOUR_DF: pd.DataFrame = pd.read_csv(f\"{RAW_DATA_DIR}/test/behaviours.tsv\", sep='\\t', header=None, index_col=0)\n",
    "TEST_BEHAVIOURS: List[Dict] = process_behaviour_df(df=TEST_BEHAVIOUR_DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76c85f2a-7a28-4e9e-a3b9-b77d6a166141",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3937069/2586730619.py:37: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(pseudo_post_row, ignore_index=True)\n",
      "Processing posts... : 137764it [17:52, 128.47it/s]\n",
      "/tmp/ipykernel_3937069/2586730619.py:37: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(pseudo_post_row, ignore_index=True)\n",
      "Processing posts... : 77365it [10:11, 126.59it/s]\n",
      "/tmp/ipykernel_3937069/2586730619.py:37: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(pseudo_post_row, ignore_index=True)\n",
      "Processing posts... : 70402it [09:11, 127.55it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from typing import List, Dict\n",
    "\n",
    "TRAIN_POST_DF: pd.DataFrame = pd.read_csv(f\"{RAW_DATA_DIR}/train/news.tsv\", sep='\\t', header=None, index_col=0)\n",
    "TRAIN_POST_ID_TO_INFO: Dict[int, Dict] = process_post_df(df=TRAIN_POST_DF, is_training=True)\n",
    "\n",
    "DEV_POST_DF: pd.DataFrame = pd.read_csv(f\"{RAW_DATA_DIR}/dev/news.tsv\", sep='\\t', header=None, index_col=0)\n",
    "DEV_POST_ID_TO_INFO: Dict[int, Dict] = process_post_df(df=DEV_POST_DF, is_training=False)\n",
    "\n",
    "TEST_POST_DF: pd.DataFrame = pd.read_csv(f\"{RAW_DATA_DIR}/test/news.tsv\", sep='\\t', header=None, index_col=0)\n",
    "TEST_POST_ID_TO_INFO: Dict[int, Dict] = process_post_df(df=TEST_POST_DF, is_training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b19eba1-07ca-455b-b849-a01e778dbe20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading word embedding data...: 1587507it [05:01, 5261.48it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THERE ARE 37364 WORDS OVER 79478 WORDS HAVE PRETRAINED EMBEDDING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "WORD_EMBEDDING = load_word_embedding()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589593c2-cf54-4fcf-9c72-f05388363fe9",
   "metadata": {},
   "source": [
    "## SAVE DATA ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11f61349-0003-4224-8c19-86f433c59a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN BEHAVIOURS SIZE: 363837;   DEV SIZE: 113207;    TEST SIZE: 108171\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from common.utils import PickleWriteObjectToLocalPatient\n",
    "\n",
    "\n",
    "print(f\"TRAIN BEHAVIOURS SIZE: {len(TRAIN_BEHAVIOURS)};   DEV SIZE: {len(DEV_BEHAVIOURS)};    TEST SIZE: {len(TEST_BEHAVIOURS)}\")\n",
    "\n",
    "PickleWriteObjectToLocalPatient().write(x=TRAIN_BEHAVIOURS, file_name=f\"{CLEAN_DATA_DIR}/train/behaviours.pkl\")\n",
    "PickleWriteObjectToLocalPatient().write(x=DEV_BEHAVIOURS, file_name=f\"{CLEAN_DATA_DIR}/dev/behaviours.pkl\")\n",
    "PickleWriteObjectToLocalPatient().write(x=TEST_BEHAVIOURS, file_name=f\"{CLEAN_DATA_DIR}/test/behaviours.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "375bfa2c-cc3f-46c6-b626-b547b4c691fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL USERS: 113282, TOTAL POSTS: 169003\n",
      "NUMBER OF CATEGORIES: 55; NUMBER OF WORDS: 79478\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from common.utils import PickleWriteObjectToLocalPatient\n",
    "\n",
    "\n",
    "print(f\"TOTAL USERS: {len(USER_TO_INT)}, TOTAL POSTS: {len(POST_TO_INT)}\")\n",
    "PickleWriteObjectToLocalPatient().write(x=USER_TO_INT, file_name=f\"{CLEAN_DATA_DIR}/object_to_int/user_to_int.pkl\")\n",
    "PickleWriteObjectToLocalPatient().write(x=POST_TO_INT, file_name=f\"{CLEAN_DATA_DIR}/object_to_int/post_to_int.pkl\")\n",
    "print(f\"NUMBER OF CATEGORIES: {len(CATEGORY_TO_INT)}; NUMBER OF WORDS: {len(WORD_TO_INT)}\")\n",
    "PickleWriteObjectToLocalPatient().write(x=CATEGORY_TO_INT, file_name=f\"{CLEAN_DATA_DIR}/object_to_int/category_to_int.pkl\")\n",
    "PickleWriteObjectToLocalPatient().write(x=WORD_TO_INT, file_name=f\"{CLEAN_DATA_DIR}/object_to_int/word_to_int.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d5c513e4-9ef8-4559-bc43-aa623d3a4ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUMBER OF POSTS IN TRAIN SET: 135304; DEV SET: 76335; TEST SET: 69782\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from common.utils import PickleWriteObjectToLocalPatient\n",
    "\n",
    "\n",
    "print(f\"NUMBER OF POSTS IN TRAIN SET: {len(TRAIN_POST_ID_TO_INFO)}; DEV SET: {len(DEV_POST_ID_TO_INFO)}; TEST SET: {len(TEST_POST_ID_TO_INFO)}\")\n",
    "PickleWriteObjectToLocalPatient().write(x=TRAIN_POST_ID_TO_INFO, file_name=f\"{CLEAN_DATA_DIR}/train/post_id_to_info.pkl\")\n",
    "PickleWriteObjectToLocalPatient().write(x=DEV_POST_ID_TO_INFO, file_name=f\"{CLEAN_DATA_DIR}/dev/post_id_to_info.pkl\")\n",
    "PickleWriteObjectToLocalPatient().write(x=TEST_POST_ID_TO_INFO, file_name=f\"{CLEAN_DATA_DIR}/test/post_id_to_info.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e36fe3b4-b3c3-499f-8d48-8e7c723f819b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from common.utils import PickleWriteObjectToLocalPatient\n",
    "\n",
    "\n",
    "PickleWriteObjectToLocalPatient().write(x=WORD_EMBEDDING, file_name=f\"{CLEAN_DATA_DIR}/pretrained/title_abstract_word_embedding.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "6718cf4ad78129df1b1eee605c27a486cdacf4913d4ca86ea45fb6ece534069b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
